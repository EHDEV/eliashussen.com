+++
title = ""
slug = "resume"
+++


# Elias Hussen Resume

New York, NY 10029        

347-635-4276 

ehussen@gmail.com    

---

### EDUCATION

Columbia University – New York, NY			  Sep 2014 – Dec 2016               
Master of Science, Data Science 

Baruch College, NY			  Sep 2007 – Dec 2010               
BBA, Information Systems    


### SELECTED COURSEWORK:

Machine Learning | Neural Networks and Deep Learning | Probability | Statistical Inference and Modeling | Microservice Apps and APIs | Exploratory Data Analysis and Visualization | Translational Bioinformatics | Computer Systems for Data Science.

### SELECTED DATA SCIENCE PROJECTS

**The All Convolutional Net.**  Tools Used: Python, Theano, Lasagne, Scikit-Learn, Scipy, Numpy, AWS EC2 Elastic GPU

Implemented the All Convolutional Net deep learning architecture in Theano and Lasagne; recreated the results in the paper. Confirmed thesis in the paper that a pooling layer can be replaced by increasing the stride in the convolutional layers to obtain a similar accuracy in classifying cyphar10 and cyphar100 datasets. 

**Artist Recommender System Using Python and Apache Spark.**  Tools Used: PySpark, MlLib, Python, AWS EMR
Built a recommender system in PySpark using the Alternating Least Square Matrix Factorization algorithm to model listening behavior of users and recommend new artists based on implicit feedback inferred.

**Relaxation Response (RR) Induced Expression Changes to Genes Associated with Diabetes, Alzheimer's and Obesity.** Tools Used: R, 

Machine learning research on expression data of genes that are linked to Diabetes, Alzheimers and Obesity to discover those with statistically significant association to relaxation response activities. Trained Elastic Net and PCA models in R to classify among control group, long-term and short-term practitioners of RR activities. Discovered a number of genes most associated with immunity, that were significantly elevated in long-term and short-term practitioners as opposed to the control group. 

**Employee Tenure Machine Learning Challenge..** Tools Used: Python, Scikit-Learn, Numpy, Pandas, Matplotlib, Seaborn, Scipy, Jupyter 

A modeling, and data exploration challenge on real world data for an HR consulting company to analyze and make make predictions of employee's tenure length. Developed using Python, an analytics codebase to perform exploratory analysis, feature engineering and parameter tuning and choose the model with the highest cross validation score after selecting the best hyper-parameter settings for each model. 

### RECENT WORK EXPERIENCE

**Senior Data Engineer:**        Vice Media – New York, NY			  Dec 2018 – July 2019

Contributed to designing, building, and deploying high-performance production platforms to support data warehousing, real-time ETL, and batch big-data processing; help define standards and best practices for enterprise usage.
Translate business analytic needs into enterprise data models and ETL processes to populate them.
Develop compelling PoC’s for data solutions using new tools and methods for big data ingestion, processing and analytics. 


*Principal Data Engineer (Consultant):*         Bank of America – New York, NY			  Dec 2016 – June 2017

Build and maintain Python packages that consume real time data from a Java REST API, transform, and perform operations. Collaborate with engineers, testers and systems administrators to identify data and analytics requirements and develop practical solutions. 
Contribute to a record linkage Python project to collect records from multiple data sources and perform statistical analysis to identify matching records. 
Automate middleware administration tasks using Python and shell scripts. 


*Analytics Engineer:*       Unified Social – New York, NY			  Sep 2014 – Sep 2015

Successfully developed and managed an internal reporting Python project, developing the architecture, building, and deploying pipelines that collect ad statistics from Twitter, Facebook, and LinkedIn; transform using Pandas, numpy, and SQL; and pipe into a modeling and visualization platform. 
Collaborated cross-functionally in a team of Engineers, Data Scientists, and Marketing Managers to build, test, and deploy a redshift data warehouse along with required ETL pipelines. 
Processed data for analysis and reporting by developing several Apache PIG and Python scripts to extract massive amounts of unstructured data from S3 and subsequently performed transformations, custom defined operations and loaded the data into a Redshift data warehouse. 
Developed data pipelines, relational objects and SQL queries for analysis through extensive collaboration within the data science team to identify, transform and engineer features that will be fitted into predictive and optimization models. 


*Business Intelligence Analyst:*        SourceMedia*– New York, NY			   Dec 2012 – Dec 2013

Worked in tandem with marketing and sales teams to perform deep data analysis to measure and understand subscription churn and retention rates. 
Developed and deployed data pipelines, rich interactive dashboards and visualizations of marketing and financial statistics using SQL, Spotfire and Talend.


*Business Data Analyst:*         Pervasive Software – Austin, TX			    May 2011 -  Aug 2012

Owned the design, development and maintenance of an end-to-end enterprise BI platform that incorporates, data pipelines, visualizations and dashboards as well as a SQL Server data warehouse to support sales and marketing decisions. 
Integrated Pervasive’s Salesforce.com CRM, Zuora SaaS platform, and SQL Server data warehouse by developing a number of complex ETL jobs. 
Collaborated extensively with business users, application developers, and production teams across functional units to identify business data requirements and implement BI solutions.


*Production Support Analyst:*            On Deck Capital – New York, NY			    Mar 2010 – Mar 2011

Lead the production support team that supported the company's loan transaction system, CRM, and enterprise data warehouse. Working closely with the finance team, engineered daily and monthly reconciliation pipelines that collect detailed loan transaction data from disparate internal and external data sources, transform, load, analyze and generate detailed reconciliation reports.
Engineered and deployed multiple data pipelines to regularly migrate loan transaction data into an enterprise data warehouse.
Partnered with business users to prepare and analyze business requirements; subsequently generating ad hoc reports and building a financial reconciliation tool made up of SSIS ETL jobs and complex SQL scripts.


### TECHNICAL SKILLS

**Machine Learning:** Classification, Regression, Collaborative Filtering, Clustering, Feature Engineering, Model Selection, Ensemble Learning Methods, Deep Learning.

**Statistical Methods:** regression models, hypothesis testing and confidence intervals, principal component analysis and dimensionality reduction.

**Data Visualization:** Tableau, D3, HighCharts, Matplotlib, Seaborn, ggplot2.

**Software and Programming Languages:** Python (Scikit-Learn, Pandas, XGboost, Theano, Keras, Flask, Tensorflow), R, SQL, Hadoop (Hive, Pig, MapReduce), Spark (PySpark, MLlib), Oracle, Redshift, MongoDB, DynamoDB.
